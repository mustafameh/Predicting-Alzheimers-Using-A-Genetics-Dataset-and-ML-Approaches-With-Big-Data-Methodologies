{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file extracts the top 100 features each from all the splits using Random Forest Feature Extraction and creates a new csv files which aggregates Top features from all the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import os\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalFeatureSelection\") \\\n",
    "    .config(\"spark.driver.memory\", \"55g\") \\\n",
    "    .config(\"spark.executor.memory\", \"55g\") \\\n",
    "    .config(\"spark.driver.cores\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the split files\n",
    "directory = r\"C:\\Users\\hcymm3\\Desktop\\Dementia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: split_1.csv\n",
      "Processed file: split_1.csv\n",
      "Columns in binary_target_df: ['PHENOTYPE', 'rs1886651_G', 'rs1456169_C', 'rs315049_A', 'rs2816048_A', 'rs4526593_A', 'rs72637903_A', 'rs10489354_A', 'rs1413529_G', 'rs2789428_A', 'rs1384164_G', 'rs2376727_A', 'rs12028469_A', 'rs2274333_G', 'rs1342709_G', 'rs2152856_G', 'rs12406679_A', 'rs11265461_G', 'rs10493772_A', 'rs12407355_G', 'rs2518544_G', 'rs6661668_A', 'rs11205102_A', 'rs2341471_A', 'rs10922501_G', 'rs2034124_A', 'rs6702652_C', 'rs183148633_G', 'rs11210343_G', 'rs6695366_G', 'rs4026409_A', 'rs709767_A', 'rs6657429_A', 'rs7517675_A', 'rs2247560_G', 'rs11163306_A', 'rs55891684_A', 'rs198415_G', 'rs12033004_G', 'rs3766415_G', 'rs4576621_A', 'rs12023742_A', 'rs992660_A', 'rs6674337_A', 'rs12117564_G', 'rs1886632_G', 'rs712889_A', 'rs12137412_A', 'rs11102782_A', 'rs72909243_A', 'rs1997762_A', 'rs4661540_A', 'rs164989_A', 'rs12129734_G', 'rs12742405_A', 'rs1776286_A', 'rs12031064_A', 'rs10915428_A', 'rs12021758_G', 'rs876171_G', 'rs270764_G', 'rs4147798_A', 'rs10888647_A', 'rs292809_G', 'rs11348708_D', 'rs9943251_A', 'rs4655616_A', 'rs7528354_G', 'rs10902678_G', 'rs1938426_G', 'rs10874924_A', 'rs12729017_A', 'rs852802_G', 'rs12028716_G', 'rs537951_G', 'rs654165_C', 'rs1295107_A', 'rs11583200_G', 'rs6663305_G', 'rs524998_A', 'rs2764900_G', 'rs1325755_A', 'rs4997711_C', 'rs615352_G', 'rs11208755_C', 'rs10923754_G', 'rs4648727_A', 'rs3107151_A', 'rs16832866_A', 'rs2026596_G', 'rs17123151_A', 'rs10908810_G', 'rs6541035_A', 'rs4907013_G', 'rs6695172_A', 'rs12059300_A', 'rs696702_A', 'rs2209354_A', 'rs11800828_G', 'rs9436444_A', 'rs4245647_A']\n",
      "\n",
      " Number of columns in the DataFrame: 101\n",
      "DF Saved\n",
      "Processing file: split_2.csv\n",
      "DF Saved\n",
      "Processing file: split_3.csv\n",
      "DF Saved\n",
      "Processing file: split_4.csv\n",
      "DF Saved\n",
      "Processing file: split_5.csv\n",
      "DF Saved\n",
      "Processing file: split_6.csv\n",
      "DF Saved\n",
      "Processing file: split_7.csv\n",
      "DF Saved\n",
      "Processing file: split_8.csv\n",
      "DF Saved\n",
      "Processing file: split_9.csv\n",
      "DF Saved\n",
      "Processing file: split_10.csv\n",
      "DF Saved\n",
      "Processing file: split_11.csv\n",
      "DF Saved\n",
      "Processing file: split_12.csv\n",
      "DF Saved\n",
      "Processing file: split_13.csv\n",
      "DF Saved\n",
      "Processing file: split_14.csv\n",
      "DF Saved\n",
      "Processing file: split_15.csv\n",
      "DF Saved\n",
      "Processing file: split_16.csv\n",
      "DF Saved\n",
      "Processing file: split_17.csv\n",
      "DF Saved\n",
      "Processing file: split_18.csv\n",
      "DF Saved\n",
      "Processing file: split_19.csv\n",
      "DF Saved\n",
      "Processing file: split_20.csv\n",
      "DF Saved\n"
     ]
    }
   ],
   "source": [
    "# Loop through files split_1.csv to split_20.csv\n",
    "for i in range(1, 21):\n",
    "    # Construct the file path dynamically\n",
    "    file_path = os.path.join(directory, f\"split_{i}.csv\")\n",
    "\n",
    "    print(f\"Processing file: split_{i}.csv\")\n",
    "    # Read the CSV file into a Spark DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Drop columns conditionally based on the index `i`\n",
    "    if i == 1:\n",
    "        # Drop the first and second columns ( ID and SEX)\n",
    "        df = df.drop(df.columns[0], df.columns[1])\n",
    "    else:\n",
    "        # Drop only the first column (ID)\n",
    "        df = df.drop(df.columns[0])\n",
    "\n",
    "    # Get the schema of the DataFrame as a list of (column_name, data_type) tuples\n",
    "    schema = df.dtypes\n",
    "    # Identify all columns of type \"string\"\n",
    "    string_columns = [col_name for col_name, col_type in schema if col_type == \"string\"]\n",
    "\n",
    "    # Drop all string columns from the DataFrame\n",
    "    df = df.drop(*string_columns)\n",
    "\n",
    "    target_column = df.columns[0]\n",
    "    categorical_columns = df.columns[1:]\n",
    "    # Assemble features into a single vector\n",
    "    assembler = VectorAssembler(inputCols=categorical_columns, outputCol=\"features\")\n",
    "    assembled_df = assembler.transform(df)\n",
    "\n",
    "   \n",
    "    # Train a RandomForest Classifier\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=target_column, numTrees=100,seed=42)\n",
    "    model = rf.fit(assembled_df)\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "    \n",
    "    # Create a schema explicitly\n",
    "    schema = StructType([\n",
    "    StructField(\"feature\", StringType(), True),\n",
    "    StructField(\"importance\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    importances_array = model.featureImportances.toArray()\n",
    "    features_data = [(feature, float(importance)) for feature, importance in zip(categorical_columns, importances_array)]\n",
    "\n",
    "    feature_importance_df = spark.createDataFrame(features_data, schema)\n",
    "\n",
    "    # Sort the features by importance and select the top 100\n",
    "    sorted_features = feature_importance_df.orderBy(\"importance\", ascending=False)\n",
    "    top_100_features = sorted_features.limit(100)\n",
    "\n",
    "\n",
    "    # Will only add target for first split\n",
    "    top_features_list = [row['feature'] for row in top_100 _features.collect()]\n",
    "\n",
    "\n",
    "    from pyspark.sql.functions import when, col\n",
    "    import csv\n",
    "    output_file = f'bestfeaturesRF_{i}.csv'\n",
    "\n",
    "    if i == 1:\n",
    "\n",
    "        #Add target to the left for first split mot for the rest(redundant) \n",
    "        top_features_list.insert(0, target_column)\n",
    "        filtered_df = df.select(*top_features_list)\n",
    "\n",
    "        # Convert target to binary (map 1 to 0 and 2 to 1)\n",
    "        binary_target_df = filtered_df.withColumn(target_column,\n",
    "            when(col(target_column) == 1, 0).when(col(target_column) == 2, 1)\n",
    "            )\n",
    "        \n",
    "        \n",
    "        rows = binary_target_df.collect()\n",
    "       \n",
    "        columns = binary_target_df.columns\n",
    "        with open(output_file, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(columns)  \n",
    "            writer.writerows(rows)\n",
    "        \n",
    "        print(f\"Processed file: split_{i}.csv\")\n",
    "        print(\"Columns in binary_target_df:\", binary_target_df.columns)\n",
    "        print(\"\\n Number of columns in the DataFrame:\", len(binary_target_df.columns))\n",
    "    else:\n",
    "        filtered_df = df.select(*top_features_list)\n",
    "        # Collect the DataFrame to a Python list\n",
    "        rows = filtered_df.collect()\n",
    "        columns = filtered_df.columns\n",
    "        with open(output_file, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(columns)  # Write header\n",
    "            writer.writerows(rows)        \n",
    "    print(\"DF Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
