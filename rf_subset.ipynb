{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d260d729-c10d-42e1-8d75-fdd6349cfeca",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T02:40:19.451219Z",
     "iopub.status.busy": "2024-05-10T02:40:19.450947Z",
     "iopub.status.idle": "2024-05-10T02:40:21.561372Z",
     "shell.execute_reply": "2024-05-10T02:40:21.560629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/12 11:03:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from typing import cast\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, ArrayType\n",
    "\n",
    "# initialise spark session\n",
    "spark = cast(SparkSession, SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  \\\n",
    "    .config(\"spark.network.timeout\", \"1500s\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Team4-Project-Hsin-Pao-Huang\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "There are too many columns (>297k columns!) to load into spark.\n",
    "\n",
    "So we first load the file without sperating the columns, then manually parse the metadata columns from the gene ones, and transform genes data into a column of array of integers.\n",
    "\n",
    "We also need to do some preliminary data cleaning and transforming, like transforming sex from 1 and 2 to 0 and 1, as well as using 3 to represent missing data (marked as \"NA\" in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8833ad4-d04e-4867-8e50-febd6917f94f",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T02:40:21.564850Z",
     "iopub.status.busy": "2024-05-10T02:40:21.564296Z",
     "iopub.status.idle": "2024-05-10T02:40:21.572095Z",
     "shell.execute_reply": "2024-05-10T02:40:21.571503Z"
    }
   },
   "outputs": [],
   "source": [
    "column_names = ['FID', 'IID', 'PAT', 'MAT', 'SEX', 'PHENOTYPE', 'DATA']\n",
    "\n",
    "def parse_data(spark: SparkSession):\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load('../data/JanBDRcount.raw')\n",
    "\n",
    "    df = df.withColumnRenamed(df.schema.names[0], 'raw_data')\n",
    "    df = df.select(F.split('raw_data', ' ', limit=7).alias('raw_data'))\n",
    "\n",
    "    parsed_df = df.withColumns({\n",
    "        column_names[0]: df['raw_data'][0], # FID\n",
    "        column_names[1]: df['raw_data'][1], # IID\n",
    "        column_names[2]: df['raw_data'][2].cast(IntegerType()), # PAT\n",
    "        column_names[3]: df['raw_data'][3].cast(IntegerType()), # MAT\n",
    "        column_names[4]: df['raw_data'][4].cast(IntegerType()), # SEX\n",
    "\n",
    "        # tranform from 1, 2 to 0, 1\n",
    "        column_names[5]: df['raw_data'][5].cast(IntegerType()) - F.lit(1), # PHENOTYPE\n",
    "\n",
    "        # clean up the data by using `3` to represent `NA`\n",
    "        column_names[6]: F.split(F.regexp_replace(df['raw_data'][6], 'NA', '3'), ' ').cast(ArrayType(IntegerType())), # DATA\n",
    "    })\n",
    "\n",
    "    parsed_df = parsed_df.drop('PAT', 'MAT', 'IID', 'raw_data')\n",
    "\n",
    "    return df, parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:40:21.574812Z",
     "iopub.status.busy": "2024-05-10T02:40:21.574289Z",
     "iopub.status.idle": "2024-05-10T02:40:23.158511Z",
     "shell.execute_reply": "2024-05-10T02:40:23.157979Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "df_path = '../data/df'\n",
    "parsed_df_path = '../data/parsed_df'\n",
    "\n",
    "# load parsed data from previous run\n",
    "if path.exists(df_path) and path.exists(parsed_df_path):\n",
    "    df = spark.read.load(df_path)\n",
    "    parsed_df = spark.read.load(parsed_df_path)\n",
    "else:\n",
    "    df, parsed_df = parse_data(spark)\n",
    "\n",
    "    # save the parsed data to save some time on future runs\n",
    "    df.write.save(df_path)\n",
    "    parsed_df.write.save(parsed_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Pipeline\n",
    "The genomes comes in 0, 1, 2 (and 3 for \"NA\"), so we need to one-hot encode this data first.\n",
    "\n",
    "Since the data is stored in a single column as an array of integers, it is difficult to make it work with built-in transformers like OneHotEncoder.\n",
    "\n",
    "So we've opted to write our own UDF for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1016ad7-b4c1-460b-afc2-6c66c1140700",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T02:40:23.161152Z",
     "iopub.status.busy": "2024-05-10T02:40:23.160985Z",
     "iopub.status.idle": "2024-05-10T02:40:23.402328Z",
     "shell.execute_reply": "2024-05-10T02:40:23.401947Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "# one-hot encodes columns which we want to train and assemble them into a flattened array\n",
    "class FeaturesTransformer(Transformer):\n",
    "    def _transform(self, df: DataFrame):\n",
    "        encoder = F.udf(\n",
    "            self._one_hot_encode,\n",
    "            ArrayType(IntegerType(), containsNull=False),\n",
    "        )\n",
    "\n",
    "        new_df = df.withColumn('FEATURES', encoder('SEX', 'SLICED_DATA'))\n",
    "        return new_df.withColumn('FEATURES', array_to_vector(F.col('FEATURES')))\n",
    "\n",
    "    def _one_hot_encode(self, sex: int, data: list[int]):\n",
    "        output = [sex]\n",
    "\n",
    "        for element in data:\n",
    "            vec = [0, 0, 0, 0]\n",
    "            vec[element] = 1\n",
    "            output += vec\n",
    "\n",
    "        return output\n",
    "\n",
    "features_transformer = FeaturesTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:40:23.404593Z",
     "iopub.status.busy": "2024-05-10T02:40:23.404431Z",
     "iopub.status.idle": "2024-05-10T02:40:23.685187Z",
     "shell.execute_reply": "2024-05-10T02:40:23.684637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[FID: string, SEX: int, PHENOTYPE: int, DATA: array<int>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "\n",
    "train, test = parsed_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:40:23.687190Z",
     "iopub.status.busy": "2024-05-10T02:40:23.686775Z",
     "iopub.status.idle": "2024-05-10T02:40:23.698131Z",
     "shell.execute_reply": "2024-05-10T02:40:23.697589Z"
    }
   },
   "outputs": [],
   "source": [
    "length = 3000 # number of features per slice\n",
    "current = 0 # current index, if resuming training, change to (index of last saved model + 1)\n",
    "\n",
    "# generates the indices used to slice the data, based on the length and current set above\n",
    "def indices_generator(length: int, current: int):\n",
    "    arr_length = parsed_df.select(F.array_size('DATA').alias('length')).take(1)[0]['length']\n",
    "\n",
    "    while current * length <= arr_length:\n",
    "        yield current, (current * length) + 1\n",
    "        current += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Part 1\n",
    "Slice the DATA array, and train a model on each slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:40:23.700762Z",
     "iopub.status.busy": "2024-05-10T02:40:23.700251Z",
     "iopub.status.idle": "2024-05-10T04:21:27.521834Z",
     "shell.execute_reply": "2024-05-10T04:21:27.521523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 0 completed, result: 0.5327680621798269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 1 completed, result: 0.5732202791026321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 2 completed, result: 0.5779014308426074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 3 completed, result: 0.48277689454160044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 4 completed, result: 0.602543720190779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 5 completed, result: 0.5460166048401343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 6 completed, result: 0.5937113584172408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 7 completed, result: 0.5765765765765766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 8 completed, result: 0.409468291821233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 9 completed, result: 0.5099805688040981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 10 completed, result: 0.44744744744744747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 11 completed, result: 0.5905317081787669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 12 completed, result: 0.47465112170994517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 13 completed, result: 0.46511217099452395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 14 completed, result: 0.5073308602720368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 15 completed, result: 0.518459636106695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 16 completed, result: 0.43985161632220454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 17 completed, result: 0.5758699876346935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 18 completed, result: 0.46970499911676383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 19 completed, result: 0.5972443031266561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 20 completed, result: 0.5472531354884297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 21 completed, result: 0.5841724077018194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 22 completed, result: 0.5195195195195195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 23 completed, result: 0.5313548842960607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 24 completed, result: 0.5908850026497086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 25 completed, result: 0.46846846846846846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 26 completed, result: 0.5267620561738209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 27 completed, result: 0.555908850026497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 28 completed, result: 0.5156332803391627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 29 completed, result: 0.4857798975446035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 30 completed, result: 0.4857798975446034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 31 completed, result: 0.4317258434905494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 32 completed, result: 0.4674085850556438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 33 completed, result: 0.5274686451157039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 34 completed, result: 0.6225048577989754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 35 completed, result: 0.48330683624801274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 36 completed, result: 0.5322381204734146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 37 completed, result: 0.49284578696343406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 38 completed, result: 0.5110404522169228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 39 completed, result: 0.5424836601307189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 40 completed, result: 0.6074898427839603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 41 completed, result: 0.555202261084614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 42 completed, result: 0.5919448860625331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 43 completed, result: 0.5467231937820173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 44 completed, result: 0.47359123829712063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 45 completed, result: 0.5642112700936232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 46 completed, result: 0.5438968380144851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 47 completed, result: 0.5788729906376965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 48 completed, result: 0.5447800741918389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 49 completed, result: 0.5214626391096979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 50 completed, result: 0.44303126656067837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 51 completed, result: 0.5297650591768239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 52 completed, result: 0.5790496378731673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 53 completed, result: 0.4518636283342165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 54 completed, result: 0.509273979862215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 55 completed, result: 0.5036212683271506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 56 completed, result: 0.5188129305776363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 57 completed, result: 0.5380674792439499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 58 completed, result: 0.46511217099452395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 59 completed, result: 0.5249955838191132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 60 completed, result: 0.5458399576046635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 61 completed, result: 0.5910616498851794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 62 completed, result: 0.5089206853912736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 63 completed, result: 0.5486663133721957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 64 completed, result: 0.5117470411588059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 65 completed, result: 0.5292351174704115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 66 completed, result: 0.510863804981452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 67 completed, result: 0.6101395513160219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 68 completed, result: 0.5264087617028793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 69 completed, result: 0.6046634870164281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 70 completed, result: 0.5234057586998763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 71 completed, result: 0.5892951775304716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 72 completed, result: 0.5433668963080727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 73 completed, result: 0.5617382087970324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 74 completed, result: 0.48171701112877585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 75 completed, result: 0.48383677795442503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 76 completed, result: 0.5251722310545841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 77 completed, result: 0.5283518812930577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 78 completed, result: 0.4329623741388447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 79 completed, result: 0.4474474474474474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 80 completed, result: 0.5151033386327505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 81 completed, result: 0.5294117647058824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 82 completed, result: 0.516163222045575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 83 completed, result: 0.4864864864864865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 84 completed, result: 0.5143967496908673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 85 completed, result: 0.5352411234764177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 86 completed, result: 0.5400105988341283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 87 completed, result: 0.5133368662780426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 88 completed, result: 0.5574986751457339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 89 completed, result: 0.4942589648472001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 90 completed, result: 0.5020314432079137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 91 completed, result: 0.4935523759053171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 92 completed, result: 0.5945945945945946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 93 completed, result: 0.6437025260554673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 94 completed, result: 0.5290584702349408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 95 completed, result: 0.5200494612259318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 96 completed, result: 0.5887652358240594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 97 completed, result: 0.4773008302420067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 98 completed, result: 0.5066242713301538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice 99 completed, result: 0.5693340399222752\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "# uncomment the line above to skip training part 1\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='PHENOTYPE')\n",
    "\n",
    "# collect results for analysis\n",
    "results = list[tuple[int, float]]()\n",
    "\n",
    "for i, idx in indices_generator(length, current):\n",
    "    sliced_train = train.withColumn('SLICED_DATA', F.slice('DATA', idx, length)).drop('DATA')\n",
    "    sliced_test = test.withColumn('SLICED_DATA', F.slice('DATA', idx, length)).drop('DATA')\n",
    "\n",
    "    classifier = RandomForestClassifier(\n",
    "        labelCol='PHENOTYPE',\n",
    "        featuresCol='FEATURES',\n",
    "        seed=24,\n",
    "    )\n",
    "\n",
    "    # parameter tuning, could use more improvments\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classifier.maxDepth, [5, 7]) \\\n",
    "        .addGrid(classifier.numTrees, [10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "    validator = CrossValidator(\n",
    "        estimator=classifier,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[features_transformer, validator])\n",
    "    model = pipeline.fit(sliced_train)\n",
    "\n",
    "    # evaluate the model\n",
    "    predictions = model.transform(sliced_test)\n",
    "    result = evaluator.evaluate(predictions)\n",
    "\n",
    "    # save the best model, so that there's no need to retrain it every time\n",
    "    model.stages[1].bestModel.write().overwrite().save(f'../models/model_{i}')\n",
    "\n",
    "    # unpersist temporary dataframes to reduce memory usage\n",
    "    sliced_train.unpersist()\n",
    "    sliced_test.unpersist()\n",
    "\n",
    "    results.append((i, result))\n",
    "\n",
    "    print(f'Slice {i} completed, result: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_df = spark.createDataFrame(results, schema=('model_index', 'model_auroc'))\n",
    "results_df.write.save('../data/part_1_results_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training part 2\n",
    "Generate predictions using the models from the previous part, and aggregate those predictions\n",
    "Then, train another model using those aggregated predictions to generate the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# aggregated predictions\n",
    "predictions_df = parsed_df.drop('DATA')\n",
    "predictions_df.cache()\n",
    "\n",
    "# load each model from part 1\n",
    "for i, idx in indices_generator(length, current):\n",
    "    sliced = parsed_df.withColumn('SLICED_DATA', F.slice('DATA', idx, length)).drop('DATA')\n",
    "\n",
    "    # load the model and generate predictions\n",
    "    rf_model = RandomForestClassificationModel.load(f'../models/model_{i}')\n",
    "    model = PipelineModel(stages=[features_transformer, rf_model])\n",
    "\n",
    "    predictions = model.transform(sliced) \\\n",
    "        .select('FID', 'prediction') \\\n",
    "        .withColumnRenamed('prediction', f'model_{i}')\n",
    "\n",
    "    # add predictions to predictions_df as a new column\n",
    "    predictions_df = predictions_df.join(predictions, on='FID')\n",
    "\n",
    "    # unpersist temporary dataframes to reduce memory usage\n",
    "    sliced.unpersist()\n",
    "    predictions.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 11:38:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/05/12 11:41:22 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "24/05/12 11:41:30 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:41:37 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:41:45 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:41:45 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:41:52 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:41:59 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:05 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:12 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:19 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:42:26 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:33 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:40 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:47 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:47 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:42:54 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:43:00 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:43:07 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:43:14 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:43:20 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:43:27 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:43:34 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:43:41 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:43:41 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:43:48 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:43:54 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:44:01 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:44:07 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:44:14 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:44:20 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:44:26 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:44:34 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:44:41 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:44:48 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:44:48 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:44:54 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:45:01 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:45:07 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:45:14 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:45:20 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:45:27 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:45:34 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:45:41 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:45:50 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:45:57 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:46:04 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:46:05 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:46:11 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:46:18 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:46:24 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:46:30 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:46:37 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:46:44 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:46:51 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:46:58 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:47:04 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:47:05 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:47:11 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:47:18 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:47:24 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:47:31 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:47:37 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:47:45 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:47:52 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:47:59 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:47:59 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:48:05 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:48:12 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:48:18 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:48:25 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:48:31 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:48:38 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:48:44 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:48:51 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:48:58 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:49:05 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:49:06 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:49:12 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:49:18 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:49:25 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:49:31 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:49:38 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:49:44 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:49:51 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:49:58 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:50:08 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:15 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:22 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:23 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:23 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:29 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:36 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:42 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:49 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:50:55 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:51:02 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:09 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:16 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:23 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:24 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:24 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:30 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:37 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:51:43 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:51:50 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:51:56 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:52:04 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:52:11 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:18 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:18 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:19 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:25 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:31 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:38 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:44 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:52:51 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:52:57 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:53:04 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:53:11 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:53:18 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:53:25 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:53:26 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:53:26 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:53:32 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:53:39 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:53:45 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:53:52 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:53:58 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:54:05 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:54:12 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:54:19 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:54:27 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:54:34 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:54:34 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:54:41 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:54:48 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:54:54 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:55:00 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 11:55:07 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:55:14 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:55:20 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/05/12 11:58:06 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "24/05/12 11:58:14 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# train test split for part 2\n",
    "train2, test2 = predictions_df.randomSplit([0.7, 0.3], seed=24)\n",
    "train2.cache()\n",
    "test2.cache()\n",
    "\n",
    "# train a new model using the aggregated predictions\n",
    "model_cols = [col for col in predictions_df.columns if col.startswith('model_') ]\n",
    "assembler = VectorAssembler(inputCols=model_cols, outputCol='FEATURES')\n",
    "\n",
    "classifier2 = RandomForestClassifier(\n",
    "    labelCol='PHENOTYPE',\n",
    "    featuresCol='FEATURES',\n",
    "    seed=42,\n",
    ")\n",
    "paramGrid2 = ParamGridBuilder() \\\n",
    "    .addGrid(classifier2.maxDepth, [5, 7]) \\\n",
    "    .addGrid(classifier2.numTrees, [10, 20]) \\\n",
    "    .build()\n",
    "evaluator2 = BinaryClassificationEvaluator(labelCol='PHENOTYPE')\n",
    "validator2 = CrossValidator(\n",
    "    estimator=classifier2,\n",
    "    estimatorParamMaps=paramGrid2,\n",
    "    evaluator=evaluator2,\n",
    ")\n",
    "\n",
    "pipeline2 = Pipeline(stages=[assembler, validator2])\n",
    "model2 = pipeline2.fit(train2)\n",
    "\n",
    "# evaluate the model\n",
    "predictions2 = model2.transform(test2)\n",
    "result = evaluator2.evaluate(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 0.933624295326423\n"
     ]
    }
   ],
   "source": [
    "print(f'Result: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 11:58:23 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2.where('prediction = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final model\n",
    "model2.stages[1].bestModel.save('../models/final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 12:02:53 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions2.select('FID', 'PHENOTYPE', 'prediction').write.save('../data/predictions2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 12:02:34 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 12:02:34 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "24/05/12 12:02:35 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+----------+\n",
      "|                FID|PHENOTYPE|prediction|\n",
      "+-------------------+---------+----------+\n",
      "|201904690143_R02C02|        1|       1.0|\n",
      "|202136020022_R07C01|        1|       1.0|\n",
      "|201058890047_R07C01|        1|       1.0|\n",
      "|201039770046_R03C02|        1|       1.0|\n",
      "|201904690139_R12C02|        0|       0.0|\n",
      "|202136020014_R07C01|        1|       1.0|\n",
      "|201904690030_R01C02|        1|       1.0|\n",
      "|202136020008_R04C01|        0|       0.0|\n",
      "|202136020022_R07C02|        0|       0.0|\n",
      "|201039780020_R11C02|        1|       1.0|\n",
      "|202062520039_R11C01|        1|       1.0|\n",
      "|201039770044_R04C02|        1|       1.0|\n",
      "|201023670027_R04C01|        1|       1.0|\n",
      "|201023680016_R10C02|        1|       1.0|\n",
      "|201023680028_R08C02|        1|       1.0|\n",
      "|201039780023_R06C02|        1|       0.0|\n",
      "|201039770053_R10C01|        1|       1.0|\n",
      "|201039770133_R09C02|        1|       1.0|\n",
      "|201904690142_R12C02|        1|       1.0|\n",
      "|201039780020_R09C01|        1|       1.0|\n",
      "+-------------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions2.select('FID', 'PHENOTYPE', 'prediction').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Subdataset + Random Forest Cloned - Hsin-Pao Huang",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
